{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write files form source to new directory, consolidate into one dir\n",
    "main_file_path = '/Users/sversage/Desktop/'\n",
    "data_dir = \"Sign-Language-and-Static-gesture-recognition-using-sklearn/dataset/\"\n",
    "new_data_dir = \"CNNSignLanguage/data\"\n",
    "\n",
    "\n",
    "def move_from_dir_collected():\n",
    "    for file_ in os.listdir(os.path.join(main_file_path, data_dir)):\n",
    "        if file_.startswith('u'):\n",
    "            pic_dir = os.path.join(main_file_path, data_dir, file_)\n",
    "            for f2 in os.listdir(pic_dir):\n",
    "                if '.csv' not in f2:\n",
    "                    destination = os.path.join(main_file_path, new_data_dir, 'temp',\"{}{}\".format(file_[-1], f2))\n",
    "                    origin = os.path.join(pic_dir, f2)              \n",
    "                    shutil.move(origin, destination)\n",
    "\n",
    "def split_data_train_dev():\n",
    "    #train test split the pictures\n",
    "    pics = os.listdir('./data/temp/')\n",
    "    np.random.shuffle(pics)\n",
    "\n",
    "\n",
    "    for pic in pics[:-150]:\n",
    "        if os.path.isdir(pic):\n",
    "            print(pic)\n",
    "            continue\n",
    "            print(pic)\n",
    "        shutil.move(\"./data/temp/{}\".format(pic), \"./data/train/\")\n",
    "\n",
    "    for pic in pics[-150:]:\n",
    "        if os.path.isdir(pic):\n",
    "            continue\n",
    "        shutil.move(\"./data/temp/{}\".format(pic), \"./data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_from_dir_collected()\n",
    "split_data_train_dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(infilename) :\n",
    "    img = Image.open(infilename)\n",
    "    img.load()\n",
    "    data = np.asarray(img, dtype=\"int32\")\n",
    "    return data\n",
    "\n",
    "def save_image( npdata, outfilename ) :\n",
    "    img = Image.fromarray(np.asarray( np.clip(npdata,0,255), dtype=\"uint8\"), \"L\" )\n",
    "    img.save(outfilename)\n",
    "    \n",
    "def load_train_test():\n",
    "    \n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    test_X = []\n",
    "    test_y = []\n",
    "    \n",
    "    for pic in os.listdir('data/train/'):\n",
    "        train_X.append(load_image('data/train/{}'.format(pic)))\n",
    "        train_y.append(pic[1])\n",
    "    \n",
    "    for pic in os.listdir('data/test/'):\n",
    "        test_X.append(load_image('data/test/{}'.format(pic)))\n",
    "        test_y.append(pic[1])\n",
    "    \n",
    "    \n",
    "    return np.array(train_X), np.array(train_y), np.array(test_X), np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = load_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample, img_height, img_width, n_dims = train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MultiLabelBinarizer()\n",
    "encoder.fit(train_y)\n",
    "y_train = encoder.transform(train_y)\n",
    "classes = y_train.shape[1]\n",
    "\n",
    "y_test = encoder.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                    rotation_range=20,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='nearest')\n",
    "\n",
    "train_gen = train_datagen.flow(train_X, y_train)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen = test_datagen.flow(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN(classes, input_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "#     model.add(Input((240,320,3)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((3, 3), input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.15))\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = model.add(Conv2D(32, (7, 7), strides = (1, 1), name = 'conv1'))\n",
    "    X = model.add(BatchNormalization(axis = 3, name = 'bn1'))\n",
    "    X = model.add(Activation('relu'))\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = model.add(MaxPooling2D((2, 2), name='max_pool2'))\n",
    "\n",
    "    model.add(Conv2D(64, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sversage/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    }
   ],
   "source": [
    "my_cnn = build_CNN(classes, (240,320,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1530/1530 [==============================] - 678s 443ms/step - loss: 3.4831 - acc: 0.1235\n",
      "Epoch 2/5\n",
      "1530/1530 [==============================] - 613s 400ms/step - loss: 1.3541 - acc: 0.6314\n",
      "Epoch 3/5\n",
      "1530/1530 [==============================] - 598s 391ms/step - loss: 0.2731 - acc: 0.9268\n",
      "Epoch 4/5\n",
      "1530/1530 [==============================] - 710s 464ms/step - loss: 0.0633 - acc: 0.9850\n",
      "Epoch 5/5\n",
      "1530/1530 [==============================] - 898s 587ms/step - loss: 0.0240 - acc: 0.9974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13d6784e0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cnn.fit(x=X_train, y=y_train, epochs=5, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 30s 199ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.5172802035013833, 0.71333333174387614]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.json', 'w') as f:\n",
    "    f.write(my_cnn.to_json())\n",
    "\n",
    "my_cnn.save_weights('my_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"my_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B',)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('./data/mA0.jpg')\n",
    "img = img.resize((320,240))\n",
    "new = data = np.asarray(img, dtype=\"int32\")\n",
    "new_y = encoder.transform(['A'])\n",
    "\n",
    "encoder.inverse_transform(my_cnn.predict(new[None,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HappyModel(input_shape, classes):\n",
    "    \"\"\"\n",
    "    Implementation of the HappyModel.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool1')(X)\n",
    "    \n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool2')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(24, activation='softmax', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MultiLabelBinarizer()\n",
    "encoder.fit(train_y)\n",
    "y_train = encoder.transform(train_y)\n",
    "classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "happyModel = HappyModel((240,320,3), classes=classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "happyModel.compile(optimizer='adam', metrics=['accuracy'], loss=['sparse_categorical_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected fc to have shape (1,) but got array with shape (24,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b021018b58d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### START CODE HERE ### (1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhappyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected fc to have shape (1,) but got array with shape (24,)"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "happyModel.fit(x=X_train, y=y_train, epochs=5, batch_size=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
